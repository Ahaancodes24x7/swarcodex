# ============================================
# SWAR AI Configuration - Environment Variables
# ============================================
# Copy this file to .env.local and fill in your settings
# DO NOT commit this file to git

# ============================================
# LLM Provider Selection
# ============================================
# Choose ONE of the following options:
# - local (Ollama - Recommended, FREE, PRIVATE)
# - openai (GPT-3.5-turbo or GPT-4)
# - gemini (Google Gemini Pro)
# - claude (Anthropic Claude 3)

VITE_LLM_PROVIDER=local

# ============================================
# OPTION 1: Local Ollama Setup
# (Recommended - Private, Free, Offline)
# ============================================
# Install: https://ollama.ai
# Run: ollama pull mistral && ollama serve

VITE_LOCAL_MODEL_URL=http://localhost:11434/api
VITE_LLM_MODEL=mistral

# ============================================
# OPTION 2: OpenAI
# (Most Capable, Requires API Key - Paid)
# ============================================
# Get API key: https://platform.openai.com/api-keys
# Uncomment below and set VITE_LLM_PROVIDER=openai

# VITE_LLM_API_KEY=sk-your-openai-api-key-here
# VITE_LLM_MODEL=gpt-3.5-turbo
# VITE_LLM_TEMPERATURE=0.3
# VITE_LLM_MAX_TOKENS=1000

# ============================================
# OPTION 3: Google Gemini
# (Free Tier Available, Easy Setup)
# ============================================
# Get API key: https://makersuite.google.com/app/apikey
# Uncomment below and set VITE_LLM_PROVIDER=gemini

# VITE_LLM_API_KEY=your-gemini-api-key-here
# VITE_LLM_MODEL=gemini-pro
# VITE_LLM_TEMPERATURE=0.3
# VITE_LLM_MAX_TOKENS=1000

# ============================================
# OPTION 4: Anthropic Claude
# (Most Thoughtful, Requires API Key - Paid)
# ============================================
# Get API key: https://console.anthropic.com/
# Uncomment below and set VITE_LLM_PROVIDER=claude

# VITE_LLM_API_KEY=sk-ant-your-api-key-here
# VITE_LLM_MODEL=claude-3-sonnet-20240229
# VITE_LLM_TEMPERATURE=0.3
# VITE_LLM_MAX_TOKENS=1000

# ============================================
# Feature Flags
# ============================================
# Enable/disable specific AI features

# Use LLM for semantic answer validation (highly recommended)
VITE_ENABLE_LLM_VALIDATION=true

# Enable adaptive difficulty adjustment based on performance
VITE_ENABLE_ADAPTIVE_DIFFICULTY=true

# Generate detailed session analysis reports
VITE_ENABLE_DETAILED_ANALYSIS=true

# Generate pedagogical feedback for each response
VITE_ENABLE_PEDAGOGICAL_FEEDBACK=true

# ============================================
# Performance Tuning
# ============================================

# Cache predictions to reduce API calls
VITE_CACHE_PREDICTIONS=true

# Process multiple analyses in batch (experimental)
VITE_BATCH_ANALYSIS=false

# Maximum concurrent LLM requests
VITE_MAX_CONCURRENT_ANALYSES=3

# ============================================
# LLM Model Parameters
# ============================================

# Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
# For assessment, use 0.2-0.4 (more consistent)
VITE_LLM_TEMPERATURE=0.3

# Max tokens: Maximum response length
# For validation: 500-1000 tokens is usually enough
VITE_LLM_MAX_TOKENS=1000

# ============================================
# Setup Instructions
# ============================================

# 1. QUICK START WITH LOCAL OLLAMA (Recommended):
#    a) Download Ollama: https://ollama.ai
#    b) Run: ollama pull mistral
#    c) Run: ollama serve (in terminal)
#    d) Keep everything below this comment, restart dev server
#    e) Start testing!

# 2. WITH OPENAI:
#    a) Sign up: https://platform.openai.com
#    b) Get API key from settings
#    c) Change VITE_LLM_PROVIDER to "openai"
#    d) Set VITE_LLM_API_KEY to your key
#    e) Uncomment OpenAI options below

# 3. WITH GOOGLE GEMINI (Free):
#    a) Go to: https://makersuite.google.com/app/apikey
#    b) Create new API key
#    c) Change VITE_LLM_PROVIDER to "gemini"
#    d) Set VITE_LLM_API_KEY to your key

# ============================================
# Testing
# ============================================

# Test your configuration by running:
# npm run dev
# 
# Then in browser console:
# import { getLLMService } from '@/lib/llmAnalysis';
# const llm = getLLMService();
# 
# Or check Session.tsx for validation logs

# ============================================
# Security Notes
# ============================================

# - NEVER commit this file to git
# - NEVER share your API keys with anyone
# - For production, use secure secrets management
# - Consider using local Ollama for privacy
# - Rate limit API calls to control costs

# ============================================
# Common Issues
# ============================================

# Issue: "Cannot connect to local model"
# Solution: Make sure Ollama is running (ollama serve)

# Issue: "API key not found"
# Solution: Verify VITE_LLM_API_KEY is set and restart dev server

# Issue: "Model not found"
# Solution: Run "ollama pull mistral" or another model name

# Issue: Slow responses
# Solution: Try simpler model (neural-chat) or reduce VITE_LLM_MAX_TOKENS

# Issue: High API costs
# Solution: Use local Ollama (free) or Google Gemini (free tier)
